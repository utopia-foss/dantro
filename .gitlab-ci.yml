---
image: python:3.10

stages:
  - check
  - test
  - build
  - deploy
  - post_deploy

# Define global pipeline rules
workflow:
  rules:
    # Run pipelines on tags
    - if: $CI_COMMIT_TAG
    # Run pipelines on branches
    - if: $CI_COMMIT_BRANCH

variables:
  # The GitLab Pages URL at which build artifacts can be made available
  PAGES_URL: https://utopia-project.gitlab.io/-/dantro


# -- Hidden Jobs --------------------------------------------------------------
# ... to be integrated in other jobs via "extends"

# Add reports
#
# 1) Unit test reports, see:
#       https://docs.gitlab.com/ee/ci/unit_test_reports.html
#
# 2) Cobertura coverage report visualization for MRs.
#
#    Note that this requires a special syntax with *relative* file paths in
#    the XML that do *not* include the test files. These settings are made
#    in the pyproject.toml and tox.ini
.reports:
  coverage: '/TOTAL.*?(\d+\.?\d*)%\s*/'  # https://regex101.com/r/vW26X0/1
  artifacts:
    when: always
    expire_in: 3 months
    paths:
      - coverage.xml
      - report.xml
      - tests/_output
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
      junit: report.xml


# -- Checks -------------------------------------------------------------------

check:hooks:
  stage: check
  before_script:
    - pip install pre-commit
  script:
    - pre-commit run --all-files --verbose --show-diff-on-failure



# -- Test Stage ---------------------------------------------------------------
# ... for testing with different python environments
#
# NOTE These jobs run via GitLab SaaS, which only use a single vCPU. If more
#      virtual CPUs should become required, custom tags can be set. See:
#      https://docs.gitlab.com/ee/ci/runners/saas/linux_saas_runner.html

test:py39:
  stage: test
  image: python:3.9
  extends:
    - .reports
  needs: []
  variables:
    DANTRO_USE_TEST_OUTPUT_DIR: "yes"  # have output in tests/_output
  before_script:
    - pip3 install tox
  script:
    - tox -v -e py39

test:py310:
  extends: test:py39
  image: python:3.10
  script:
    - tox -v -e py310

test:py311:
  extends: test:py39
  image: python:3.11
  script:
    - tox -v -e py311

test:py312:
  extends: test:py39
  image: python:3.12
  script:
    - tox -v -e py312

test:py313:
  extends: test:py39
  image: python:3.13
  script:
    - tox -v -e py313

test:windows:py312:
  extends: test:py312
  image: 3.12-windowsservercore-ltsc2022
  tags:
    - saas-windows-medium-amd64
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule"
      when: always
      allow_failure: true
    - if: $CI_PIPELINE_SOURCE == "web"
      when: always
      allow_failure: true
    - if: $CI_COMMIT_TAG
      when: always
      allow_failure: true
    - when: manual
      allow_failure: true
  retry:
    max: 1
    when: script_failure
  before_script:
    - Set-Variable -Name "time" -Value (date -Format "%H:%m")
    - echo ${time}
    - echo "started by ${GITLAB_USER_NAME}"
    - CHCP 65001
    #
    - Import-Module "$env:ChocolateyInstall\helpers\chocolateyProfile.psm1"
    - choco install python --version=3.12.4 -y -f
    - refreshenv
    #
    - pip install tox


# -- Build Stage --------------------------------------------------------------
# ... for building the documentation (and potentially more jobs)

build:docs:
  image: python:3.10   # to be identical to ReadTheDocs build
  stage: build
  allow_failure: true
  needs: []
  before_script:
    - pip3 install .[dev]
  script:
    - cd doc
    - DANTRO_DOC_GENERATE_FIGURES=True make doc
    - make linkcheck
    - make doctest
  after_script:
    # Append the error log such that it's more convenient to read
    - echo "-------- Errors emitted during building of documentation --------"
    - cat doc/build_errors.log

  artifacts:
    when: always
    name: "doc-$CI_COMMIT_REF_NAME"
    expire_in: 2 weeks
    expose_as: Documentation Build Results - including error log
    paths:
      - doc/_build/html
      - doc/build_errors.log

  environment:
    name: review/docs/$CI_COMMIT_REF_NAME
    auto_stop_in: 2 months
    url: $PAGES_URL/-/jobs/$CI_JOB_ID/artifacts/doc/_build/html/index.html






# -- Deploy Stage -------------------------------------------------------------
# ... for deployment of dantro to the PyPI (and other places)

# Deploy dantro to PyPI
deploy:pypi:
  stage: deploy
  rules: &pypi_deploy_rules
    # Run after pushes to tags in original repo, not forks
    - if: $CI_COMMIT_TAG && $CI_PROJECT_PATH == "utopia-project/dantro"
  variables:
    # Define a regex for matching the tag name, see:
    #     https://regex101.com/r/AsCCJo/2
    # Expects fully-qualified version specifiers, like v1.2.3 or v1.2.3a4
    # Does NOT accept tags like:
    #     1.2.3   (missing v)
    #     v1.0    (missing patch version)
    VERSION_PATTERN: v([[:digit:]]+)\.([[:digit:]]+)\.([[:digit:]]+)([[:lower:]][[:digit:]]+)?

    # Need another matching pattern to extract a version string from the
    # __version__ line of an __init__.py file (line extracted via grep first)
    SED_PATTERN: s/.*\"([0-9]+\.[0-9]+\.[0-9]+[a-z]{0,5}[0-9]*)\".*/\1/g
  before_script:
    # Test the version pattern itself behaves as expected
    - "[[ ! \"foo\" =~ ${VERSION_PATTERN} ]]"
    - "[[ ! \"v1.2\" =~ ${VERSION_PATTERN} ]]"
    - "[[ ! \"1.2.3\" =~ ${VERSION_PATTERN} ]]"
    - "[[ \"v1.2.3\" =~ ${VERSION_PATTERN} ]]"
    - "[[ \"v1.23.4a5\" =~ ${VERSION_PATTERN} ]]"
    - "[[ \"v1.23.45a67\" =~ ${VERSION_PATTERN} ]]"

    # Retrieve the dantro version (without importing, to avoid installation)
    - export DANTRO_VERSION=v$(cat dantro/__init__.py | grep __version__ | sed -E $SED_PATTERN)
    - echo "dantro version is  ${DANTRO_VERSION}"

    # Now do the actual checks
    # ... against the selected tag
    - "[[ ${CI_COMMIT_TAG} =~ ${VERSION_PATTERN} ]]"

    # ... against the package version number
    - "[[ ${DANTRO_VERSION} =~ ${VERSION_PATTERN} ]]"

    # ... and that they are the same
    - "[[ ${DANTRO_VERSION} == ${CI_COMMIT_TAG} ]]"
    # Checks successful

    # Install dependencies needed for pushing packages
    - pip install -U twine
  script:
    # Create distribution files
    - python setup.py sdist bdist_wheel

    # Check whether description will render correctly on PyPI
    - twine check dist/*

    # Upload to the TEST PyPI index (using separate credentials)
    - twine upload --repository testpypi -u ${PYPI_TEST_USER} -p ${PYPI_TEST_PASSWORD} dist/*

    # If this worked, continue and upload to actual package index
    - twine upload -u ${PYPI_USER} -p ${PYPI_PASSWORD} dist/*


# -- Test Deploy Stage --------------------------------------------------------
# ... for testing the deployment to the PyPI (and potentially more jobs)

# Install dantro from the PyPI via pip to test automatic deployment
post_deploy:install_from_pypi:
  stage: post_deploy
  rules: *pypi_deploy_rules
  needs:
    - "deploy:pypi"
  script:
    # Install the newly deployed dantro version via PyPI. The current version
    # number is given by the commit tag without the prefixed 'v'.
    - pip install dantro==${CI_COMMIT_TAG#v}
