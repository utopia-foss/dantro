---
stages:
  - check
  - test
  - build
  - deploy
  - post_deploy

# Define global pipeline rules
workflow:
  rules:
    # Run pipelines on tags
    - if: $CI_COMMIT_TAG
    # Run pipelines on branches
    - if: $CI_COMMIT_BRANCH

variables:
  # Documentation server configuration
  DOC_HOST: root@hermes.iup.uni-heidelberg.de
  DOC_PORT: 2023      # ... forwards to dantro_doc_server container. root ok ;)
  DOC_REMOTE_BASE_DIR: /var/dantro_doc
  DOC_REMOTE_PATH: $DOC_REMOTE_BASE_DIR/$CI_COMMIT_REF_SLUG
  DOC_REMOTE_URL: https://hermes.iup.uni-heidelberg.de/dantro_doc


# -- Hidden Jobs --------------------------------------------------------------
# ... to be integrated in other jobs via "extends"

# For the deployment of the documentation, the jobs authenticate themselves
# via a keypair with the doc server. This is the read-only deployment key
# for the Utopia project, stored as SSH_PRIVATE_KEY CI/CD variable.
.ssh_access:
  before_script:
    # Instructions:
    #   https://docs.gitlab.com/ce/ci/ssh_keys/
    # Run ssh-agent (inside the build environment)
    - eval $(ssh-agent -s)

    # Add the SSH key stored in SSH_PRIVATE_KEY variable to the agent store
    # We're using tr to fix line endings which makes ed25519 keys work
    # without extra base64 encoding.
    # https://gitlab.com/gitlab-examples/ssh-private-key/issues/1#note_48526556
    - echo "$SSH_PRIVATE_KEY" | tr -d '\r' | ssh-add - > /dev/null

    # Create the SSH directory and give it the right permissions
    - mkdir -p ~/.ssh
    - chmod 700 ~/.ssh

    # Add the known hosts lists to ensure this ssh connection is the right one
    - echo "$SSH_KNOWN_HOSTS" > ~/.ssh/known_hosts
    - chmod 644 ~/.ssh/known_hosts

# Specify default tag, which is picked up both by single- and multicore runners
.default_tags:
  tags:
    - default

# Add reports
# 1) cobertura coverage report visualization for MRs
#    Includes patch against https://gitlab.com/gitlab-org/gitlab/-/issues/217664,
#    see also: https://forum.gitlab.com/t/enabling-cobertura/36918
# 2) unit test reports, see https://docs.gitlab.com/ee/ci/unit_test_reports.html
.reports:
  variables:
    SRC_DIR: dantro
  after_script:
    - sed -i "s=<source>.*${SRC_DIR}</source>=<source>./${SRC_DIR}</source>=g" coverage.xml
    - sed -i "s;filename=\";filename=\"${SRC_DIR}/;g" coverage.xml
  artifacts:
    when: always
    expire_in: 6 months
    reports:
      cobertura: coverage.xml
      junit: report.xml


# -- Checks -------------------------------------------------------------------

check:hooks:
  stage: check
  image: python:3.9
  extends:
    - .default_tags
  before_script:
    - pip install pre-commit
  script:
    - pre-commit run --all-files --verbose --show-diff-on-failure



# -- Test Stage ---------------------------------------------------------------
# ... for testing with different python environments

test:py37:
  stage: test
  image: python:3.7
  extends:
    - .default_tags
    - .reports
  needs: []
  before_script:
    - pip3 install tox
  script:
    - tox -v -e py37

test:py38:
  extends: test:py37
  image: python:3.8
  script:
    - tox -v -e py38

test:py39:
  extends: test:py37
  image: python:3.9
  script:
    - tox -v -e py39

test:py310:
  extends: test:py37
  image: python:3.10
  script:
    - tox -v -e py310


# Extended test jobs, which are only run upon release-related events.

test:py37_min:
  extends: test:py37
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /^release/ ||
           $CI_COMMIT_TAG'
    - if: '$CI_COMMIT_BRANCH =~ /^prepare-release/i ||
           $CI_MERGE_REQUEST_TITLE =~ /.*prepare\srelease.*/i'
      allow_failure: false
    - when: never
  allow_failure: true
  script:
    - tox -v -e py37-minimal_deps

test:py38_min:
  extends: test:py37_min
  image: python:3.8
  script:
    - tox -v -e py38-minimal_deps

test:py39_min:
  extends: test:py37_min
  image: python:3.9
  script:
    - tox -v -e py39-minimal_deps

test:py310_min:
  extends: test:py37_min
  image: python:3.10
  script:
    - tox -v -e py310-minimal_deps


# -- Build Stage --------------------------------------------------------------
# ... for building the documentation (and potentially more jobs)

build:docs:
  stage: build
  image: python:3.7    # NOTE Later versions have trouble with typing module
  extends: .default_tags
  allow_failure: true
  needs: []
  before_script:
    - pip3 install .[doc]
  script:
    - cd doc
    - make doc
  after_script:
    # Append the error log such that it's more convenient to read
    - echo "-------- Errors emitted during building of documentation --------"
    - cat doc/build_errors.log

  artifacts:
    when: always
    name: "doc-$CI_COMMIT_REF_NAME"
    expire_in: 2 weeks
    expose_as: Documentation Build Results - including error log
    paths:
      - doc/_build/html
      - doc/build_errors.log


# -- Deploy Stage -------------------------------------------------------------
# ... for deployment of the documentation, deployment of dantro to the PyPI
#     (and potentially more jobs)

# Deploy the documentation to a self-hosted web server
deploy:docs:
  stage: deploy
  image: &deploy_image ccees/utopia-base:v2.1  # overkill, but sshd installed
  extends:
    - .default_tags
    - .ssh_access
  dependencies:
    - build:docs
  needs:
    - build:docs
  script:
    # Create the directory on the remote, removing any prior version
    - echo "Creating empty remote directory $DOC_REMOTE_PATH ..."
    - ssh -p $DOC_PORT $DOC_HOST "rm -rf $DOC_REMOTE_PATH"
    - ssh -p $DOC_PORT $DOC_HOST "mkdir -p $DOC_REMOTE_PATH"

    # Copy the sphinx HTML output to the remote
    - echo "Uploading documentation to $DOC_REMOTE_PATH/ ..."
    - scp -P $DOC_PORT -pr doc/_build/html $DOC_HOST:$DOC_REMOTE_PATH/

    - echo "Finished. :)"

  environment:
    name: docs/$CI_COMMIT_REF_NAME
    url: $DOC_REMOTE_URL/$CI_COMMIT_REF_SLUG/html/
    on_stop: deploy:stop_docs

# Stop the deployed "docs" environment for this branch
# NOTE: This job is automatically executed if the original branch is deleted
#       (*GitLab Magic*) or someone stops the environment.
deploy:stop_docs:
  stage: deploy
  image: *deploy_image
  extends:
    - .default_tags
    - .ssh_access

  # For this to work as an environment stop action, need to set it to manual
  when: manual

  # Make this job independent of any artifacts, as they might have expired
  # when the environment stops. However, `needs` has to be the same as for the
  # environment deployment (this might be a bug?)
  dependencies: []
  needs:
    - build:docs
  variables:
    GIT_STRATEGY: none  # Don't check out the branch (might already be deleted)
  script:
    - echo "Removing remote directory $DOC_REMOTE_PATH ..."
    - ssh -p $DOC_PORT $DOC_HOST "rm -rf $DOC_REMOTE_PATH"
    - echo "It's gone. :)"

  environment:
    name: docs/$CI_COMMIT_REF_NAME
    action: stop

# Deploy dantro to PyPI
deploy:pypi:
  stage: deploy
  image: python:3.9
  extends:
    - .default_tags
    - .ssh_access
  rules: &pypi_deploy_rules
    # Run after pushes to tags in original repo, not forks
    - if: $CI_COMMIT_TAG && $CI_PROJECT_PATH == "utopia/dantro"
  script:
    # Define a regex for matching the tag name, see https://regex101.com/r/AsCCJo/2
    # Expects fully-qualified version specifiers, like v1.2.3 or v1.2.3a4
    # Does NOT accept tags like 1.2.3 (missing v) or v1.0 (missing patch version)
    - export VERSION_PATTERN="v([[:digit:]]+)\.([[:digit:]]+)\.([[:digit:]]+)([[:lower:]]\d+)?"
    # Before checking that the tag matches the expected pattern, check the regex
    # pattern with a few allowed versions.
    - "[[ \"v1.2.3\" =~ ${VERSION_PATTERN} ]]"
    - "[[ \"v1.23.4a5\" =~ ${VERSION_PATTERN} ]]"

    # Now do the actual check
    - "[[ ${CI_COMMIT_TAG} =~ ${VERSION_PATTERN} ]]"
    # Tag is of the correct form, yay!

    - pip install -U twine

    # Create distribution files
    - python setup.py sdist bdist_wheel

    # Check whether description will render correctly on PyPI
    - twine check dist/*

    # Upload to the TEST PyPI index (using separate credentials)
    - twine upload --repository testpypi -u ${PYPI_TEST_USER} -p ${PYPI_TEST_PASSWORD} dist/*

    # If this worked, continue and upload to actual package index
    - twine upload -u ${PYPI_USER} -p ${PYPI_PASSWORD} dist/*


# -- Test Deploy Stage --------------------------------------------------------
# ... for testing the deployment to the PyPI (and potentially more jobs)

# Install dantro from the PyPI via pip to test automatic deployment
post_deploy:install_from_pypi:
  stage: post_deploy
  image: python:3.9
  extends: .default_tags
  rules: *pypi_deploy_rules
  needs:
    - "deploy:pypi"
  script:
    # Install the newly deployed dantro version via PyPI. The current version
    # number is given by the commit tag without the prefixed 'v'.
    - pip install dantro==${CI_COMMIT_TAG#v}
